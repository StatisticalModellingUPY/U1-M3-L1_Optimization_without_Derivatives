{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "**IMPORTANT: DO NOT COPY OR SPLIT CELLS.** If you do, you'll mess the autograder. If need more cells to work or test things out, create a new cell. You may add as many new cells as you need.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURSE = \"StatModels_2020_q3\"\n",
    "GROUP = \"\" # Either D2A or D2B\n",
    "NAME = \"\" # Match your GitHub Classroom ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gonzalo G. Peraza Mues, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimation Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problems in statistics and machine learning can be framed as optimization problems, i.e., finding the right set of parameter values which maximizes or minimizes a function. Optimization theory is very mathematical in nature, involving the fields of multivariate calculus, linear algebra, differential equations, computer science, etc. It is also a field of very active research, many of which is focused on finding ways to efficiently train deep neural networks. \n",
    "\n",
    "Here, we will cover the very basics of optimization, and demonstrate some practical applications. Numerical optimization methods will serve us along the rest of the course, from distribution fitting up to Monte Carlo methods. Though we will not concern ourselves too much about the details of the optimization algorithms, it is important to understand, at least in broad terms, what is happening under the hood when, for example, we fit a distribution by maximizing the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex vs non-convex functions\n",
    "\n",
    "We start the discussion with single variable functions of the form $y=f(x)$. We are interested in finding the minimum value of a function. In what concerns us, we can distinguish two kind of functions, those with a single minimum (global minimum) and those with multiple minima (possible several local minima, and a global minimum). The former are called *convex functions*, while the latter are called *non-convex*. The distinction is important because many optimization methods can only guarantee finding a local minimum, and, if the function is *non-convex*, the global minimum may elude us. Since convex functions possess only one minimum, the global minimum, we can be sure the algorithm will converge to it. On the other hand, trying to find the global minimum of non-convex function can be prohibitively expensive (computationally speaking).\n",
    "\n",
    "A convex is convex if $f(x)$ is above all of its tangents, or, equivalently, for two points $A$ and $B$, $f(C)$ lies below the segment $[f(A), f(B)]$, for $A < C < B$. It can be proven that for a convex function a local minimum is also a global minimum. Then, in some sense, the minimum is unique.\n",
    "\n",
    "![A convex function. From: https://scipy-lectures.org/_images/sphx_glr_plot_convex_001.png](images/convex_f.png)\n",
    "\n",
    "This is not true for a non-convex function, notice the two minima.\n",
    "\n",
    "![A non-convex function. From: https://scipy-lectures.org/_images/sphx_glr_plot_convex_002.png](images/non-convex_f.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing without derivatives.\n",
    "\n",
    "## Brent's method for single variable functions\n",
    "\n",
    "Functions of one variable can be minimized  without derivatives by employing smart search algorithms. The most used algorithm to minimize scalar functions is the Brent's method. Let's consider the following example, from [this reference](https://www.wiwi.uni-wuerzburg.de/fileadmin/12010500/user_upload/skripte/ss10/CompEcon/cechap33.pdf):\n",
    "\n",
    "A household can consume two goods $x_1$ and $x_2$. He values the consumption\n",
    "of those goods with the joint utility function\n",
    "$$\n",
    "u(x_1, x_2) = x_1^{0.4}+(1+x_2)^{0.5}\n",
    "$$\n",
    "\n",
    "Here $x_2$ acts as a luxury good, i.e. the household will only consume $x_2$, if his available\n",
    "resources $W$ are large enough. $x_1$ on the other hand is a normal good and will always be\n",
    "consumed. Naturally, we have to assume that $x_1, x_2 \\geq 0$. With the prices for the goods\n",
    "being $p_1$ and $p_2$, the households has to solve the optimization problem\n",
    "$$\n",
    "\\underset{x_1,x_2\\geq 0}{\\text{max}} u(x_1, x_2) = x_1^{0.4}+(1+x_2)^{0.5}\n",
    "\\quad\\text{s.t.}\\quad p_1 x_1 + p_2 x_2 = W.\n",
    "$$\n",
    "Note that there is no analytical solution to this problem.\n",
    "\n",
    "First, note that this seems like a problem of two variables $x_1$ and $x_2$, but we can use the constrain to solve for $x_1$ and plug it back into the utility function, transforming the two-variable constrained problem into the unconstrained single variable problem:\n",
    "$$\n",
    "\\underset{x_2\\geq 0}{\\text{max}} \\left[ \\frac{W-p_2 x_2}{p_1} \\right]^{0.4} + (1 + x_2)^{0.5}.\n",
    "$$\n",
    "So, we have a maximization problem... But, we were talking about minimizing functions. That's fine, it turns out that maximizing a function $f(x)$ is equivalent to minimizing $-f(x)$. We just need to change the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cf065fae33296476c3c32bf917f703d",
     "grade": false,
     "grade_id": "cell-04a5e94d84729c1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the negative utility function.\n",
    "def u(x2, p1, p2, W):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbfdb615756043b2298beafd8cefd873",
     "grade": true,
     "grade_id": "cell-90289b3ae3179d99",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the function.\n",
    "x_grid = np.linspace(0, 5, 1000)\n",
    "plt.plot(x_grid, [u(x2, 1, 2, 10) for x2 in x_grid])\n",
    "plt.xlabel('$x_2$')\n",
    "plt.ylabel('$u(x_2)$')\n",
    "### BEGIN HIDDENT TESTS\n",
    "assert round(u(3, 1, 2, 10), 10) == -3.7411011266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bret's method relies on a parabolic approximation of the actual function $f(x)$. Finding the minimum of a parabola is quite easy. If the parabola is given by $a + bx + cx^2$, the its minimum is located at\n",
    "$$\n",
    "x = -\\frac{b}{2c}.\n",
    "$$\n",
    "The following figure shows how Brent's method proceeds in finding a minimum.\n",
    "![Brent's method.](images/brent.png)\n",
    "\n",
    "We start with the initial interval $[a, b]$ and compute the intersection point $x_1 = (a+b)/2$. We then compute a parabola that exactly contains the three points $(a, f(a))$, $(b, f(b))$ and $(x_1, f(x_1))$.The\n",
    "minimum of this parabola can be calculated and is denoted with $x_2$. If \n",
    "\n",
    "We then replace $b$ with $x_2$ and again compute a parabola through our new points. The method is repeated until we reach convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the parabolic approximation approach starts to converge too slowly, or the minimum leaves the desired interval, Brent's method switches to a typically slower but guaranteed method called the Golden-Search method. The Golden-Search method minimizes a one-dimensional function on the initially defined\n",
    "interval $[a, b]$. The ideas behind this method is quite similar to the one of bisection search. Golden-Search however divides the interval $[a_i, b_i]$ into two sub-intervals by using the points $x_{i,1}$ and $x_{i,2}$ with\n",
    "$$\n",
    "x_{i,j} = a_i + \\alpha_j(b_i - a_i)\\quad\\text{with}\\quad\n",
    "\\alpha_1 = \\frac{3-\\sqrt{5}}{2}\\text{ and }\\alpha_2 = 1 - \\alpha_1 = \\frac{\\sqrt{5} - 1}{2}.\n",
    "$$\n",
    "The values $\\alpha_1$ and $\\alpha_2$ are chosen in a way, such that the interval $[a_i, b_i]$ is intersected according to the golden ratio. We now compute the function values $f(x_{i,j})$ for $j = 1, 2$ and compare them. The next iteration's interval is then chosen according to\n",
    "$$\n",
    "\\begin{cases}\n",
    "a_{i+1} = a_i\\text{ and }b_{i+1}=x_{i,2} & \\text{if } f(x_{i,1} < f(x_{i,2})\\\\\n",
    "a_{i+1} = x_{i,1}\\text{ and }b_{i+1}=b_{i} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The idea behind this iteration rule is quite simple. If $f(x_{i,1}) < f(x_{i,2})$, the lower values of\n",
    "$f$ will be located near $x_{i,1}$, not $x_{i,2}$. Consequently, one chooses the interval $[a_i, x_{i,2}]$ as new iteration interval and therefore rules out the greater values of $f$.\n",
    "\n",
    "![Brent's method.](images/golden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brent's method is implement in Scipy's optimize module, as default method of the `minimize_scalar` function. Let's use it:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "result = optimize.minimize_scalar(u, bracket=(0, 2.5, 5), args=(1, 2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional bracket argument takes a 3-element tuple (a, b, c) with $f(b) < f(a), f(c)$, as to guarantee a minimum is located in the interval $[a, c]$. The result is stored in a `OptimizeResult`, in which, among other information, one can find the optimal value of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt = result.x\n",
    "print(x_opt)\n",
    "\n",
    "x_grid = np.linspace(0, 5, 1000)\n",
    "plt.plot(x_grid, [u(x2, 1, 2, 10) for x2 in x_grid])\n",
    "plt.xlabel('$x_2$')\n",
    "plt.ylabel('$u(x_2)$')\n",
    "plt.plot(x_opt, u(x_opt, 1, 2, 10), 'ro');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise**\n",
    "\n",
    "Explore the results object. Besides `x`, what else is stored there? Extract the number of iterations and the number of evaluations into the `iters` and `evals` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb8fefa75f29cc4f431ead1a260a3ef",
     "grade": false,
     "grade_id": "cell-8d45bf8f15bbe09e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2c5a965f253243b44b64a1e076fce65",
     "grade": true,
     "grade_id": "cell-fd29d857cda94756",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TEST\n",
    "assert iters == result.nit\n",
    "assert evals == result.nfev\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm finds the minimum inside the specified interval in about 11 iterations. Since the negative utility is a convex function, we were able to find the global minimum, which is harder to find in non-convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test Brent's method using non-convex functions. We start by defining the function\n",
    "$$\n",
    "(x - e^{-1})^2 + \\epsilon e^{-5(x - 0.5 - e^{-1})^2}\n",
    "$$\n",
    "For $\\epsilon=0$ the function is convex, while for $\\epsilon \\neq 0$ it becomes non-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, epsilon):\n",
    "    x_0 = np.exp(-1)\n",
    "    return (x - x_0)**2 + epsilon*np.exp(-5*(x - .5 - x_0)**2)\n",
    "\n",
    "x = np.linspace(-1, 3, 100)\n",
    "for eps in range(10):\n",
    "    plt.plot(x, f(x, eps), label=f'$\\epsilon={eps}$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise**:\n",
    "\n",
    "We want to test how the value of $\\epsilon$ influences the convergence of the Brent's method. Create a for loop to minimize the different functions above. Store the minimum values found into the `x_min` list. Plot each function together with its minimum. Use the bracket (-2, 2, 4.5). Visually check which optimization got stuck a local minimum larger than the global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31f562b5381e01e2c5813f6b4d7c843d",
     "grade": false,
     "grade_id": "cell-f12947fe3975ea47",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_min = []\n",
    "for eps in range(10):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6302051c6651cb11e6a18673391ee1a",
     "grade": true,
     "grade_id": "cell-c2082d002ad7cc8d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGGIN HIDDEN TESTS\n",
    "assert np.allclose(x_min, [0.3678794411714423, 0.12812912819302427, 0.05960911516130499, 0.01956411505088859, \n",
    "                    -0.008570917021825646, -0.03017584988638197, 1.6289592048571786, 1.650344520836931, \n",
    "                    1.6683276953386315, 1.683807271808013])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at least for some values of $\\epsilon$ (the larger ones), and using that bracket, the optimization gets trapped in a local minimum. Try changing the bracketing option in order to assert the global minimum is found in all cases. Do this is a new cell as to not change the solution to the previous exercise.\n",
    "\n",
    "In general, searching for the global minimum is a very expensive computational effort, and an active area of current research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we will be interested in optimizing functions of more than one variable. A multivariate function is a function that takes a vector of values as an input and return a single scalar as an output. Mathematically, they are described as $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$. For example, the two-variable function\n",
    "$$\n",
    "f(x, y) = x^2 + y^2 + x \\sin{y}+ y \\sin{x}\n",
    "$$\n",
    "takes as input the vector $[x, y]^T$, and outputs a single value $f(x,y)$. We can visualize two-variable functions using Matplotlib's 3D plotting capabilities. To plot the function $f(x,y)$ as a 3D surface we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x, y):\n",
    "    return x**2 + y**2 + x*np.sin(y) + y*np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create grid values along the two variables\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "\n",
    "# Meshgrid creates two matrices with X and Y coordinates\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Evaluate function on grid\n",
    "Z = f(X, Y)\n",
    "\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x,y)')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Ipython Widgets configured we can make the plot interactive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x,y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful way of visualizing two-variable functions is using contour plots. A contour plot is an 2D projection of two-variable function into the x-y plane. Lines (contours) in the contour plot define points of equal height, i.e., the function remains constant along these lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "# Draw the filled contours\n",
    "plt.contourf(X, Y, Z, levels=10, alpha=0.75)\n",
    "# Draw the contour lines\n",
    "C = plt.contour(X, Y, Z, levels=10, colors='black')\n",
    "# Add height labels\n",
    "plt.clabel(C, inline=1, fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nelder-Mead algorithm for multi-variable functions\n",
    "\n",
    "Now, onto the Nelder-Mead algorithm. The Nelder-Mead method uses a geometrical shape called a simplex as its 'vehicle' of sorts to search the domain. This is why the technique is also called the Simplex search method. In layman's terms, a simplex is the n-dimensional version of a 'triangle'. For 1 dimension, its a line, for 2 dimensions, its a triangle, for 3 dimensions, its a tetrahedron, etc. The shape doesn't have to be symmetrical/equilateral in any way. Note that an n-dimensional simplex has (n+1) vertexes.\n",
    "\n",
    "The method does not require any derivative information, which makes it suitable for problems with non-smooth functions. It is widely used to solve parameter estimation and similar statistical problems, where the function values are uncertain or subject to noise. It can also be used for problems with discontinuous functions, which occur frequently in statistics and experimental mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering the 1D case, for which the simplex is a line. One simple thing to try would be to sample two points relatively near each other, and just repeatedly take a step down away from the largest value. The obvious problem in this approach is using a fixed step size: it can't get closer to the true minima than the step size so it doesn't converge. It also spends too much time inching towards the minima when it's clear that the step size should be larger.\n",
    "\n",
    "To overcome these problems, the Nelder-Mead method dynamically adjusts the step size based off the loss of the new point. If the new point is better than any previously seen value, it expands the step size to accelerate towards the bottom. Likewise if the new point is worse it contracts the step size to converge around the minima. The algorithm is implemented in the `optimize.minimize` function. To use it, you need to pass the option `method=\"Nelder-Mead\"` to `minimize`. Let's test in a non-trivial function:\n",
    "$$\n",
    "f_1(x) = \\log \\left(1 + \\vert x\\vert^{2 + \\sin{x}}\\right)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return np.log(1 + np.abs(x)**(2 + np.sin(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Minimize requires to specify an initial guess or starting point x0\n",
    "result = optimize.minimize(f1, x0=-4, method=\"Nelder-Mead\")\n",
    "print(result)\n",
    "\n",
    "# Visualize the function\n",
    "x_min = result.x\n",
    "x_grid = np.linspace(-5, 6, 100)\n",
    "plt.plot(x_grid, f1(x_grid))\n",
    "plt.plot(x_min, f1(x_min), 'or');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the algorithm in 1D. As you can verify, the results object contains the simplex at the final iteration, in this case a line defined by two points. What we want is to the simplex at each iteration to visualize how the simplex evolves through the optimization process. In order to do this, we are going to forcibly terminate the algorithm at a specified number of iterations until convergence at store the simplex at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function provides a visualization for the Nelder-Mead algorithm in 1D. The `final_simplex` tuple of the results object contains two arrays, the first one contains the x-values of the points of the simplex, similarly, the second one contains the y-values. Animation is achieved by repeating and clearing the plot each iteration. Study the function, as you will need to modify it for two-dimensional functions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def visualize_opt_1d(func, interval, niter, method='Nelder-Mead'):\n",
    "    \"\"\" A visualization for the Nedler-Mead algorithm in 1D. It allows to explore each iteration of the algorithm.\n",
    "    INPUT:\n",
    "        - func: the function to minimize\n",
    "        - interval: interval to visualize, must contain the starting point x0, of the form (a, x0, b)\n",
    "        - niter: number of iterations to perform\n",
    "    OUPUT:\n",
    "        - An step by step visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack interval\n",
    "    a, x0, b = interval\n",
    "    assert a < x0 < b\n",
    "    \n",
    "    # Initialize figure\n",
    "    fig = plt.figure()\n",
    "    x_grid = np.linspace(a, b, 500)\n",
    "    \n",
    "    # Apply method. To have access to the iteration, do this in an\n",
    "    # artificial way: allow the algorithm to iter only once\n",
    "    converged = False\n",
    "    for i in range(niter):\n",
    "        # Run optimizer\n",
    "        result = optimize.minimize(func, x0, method=method, options={\"maxiter\": i})\n",
    "        \n",
    "        # Store the current simplex.\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(x_grid, func(x_grid))\n",
    "        simplex_x = result.final_simplex[0].ravel()\n",
    "        simplex_y = result.final_simplex[1].ravel()\n",
    "        plt.plot(simplex_x, simplex_y, 'o-', color='r')\n",
    "        plt.show()\n",
    "        sleep(0.4)\n",
    "        \n",
    "        # Check if already converged\n",
    "        if result.success:\n",
    "            print(f'Converged at {i} iterations')\n",
    "            converged = True\n",
    "            break\n",
    "    if not converged:\n",
    "        print(f'Did not converge after {niter} iterations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "visualize_opt_1d(f1, interval=(-5, -4, 6), niter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following functions with the Nedler-Mead algorithm. One has many local minima, the other has discontinuous derivatives. Implement each function and visualize the algorithm in a suitable interval. Create more cells as needed.\n",
    "$$\n",
    "f_2(x) = \\left(2 + \\frac{\\sin 50 x}{50}\\right)\\left(\\arctan x\\right)^2\\\\\n",
    "f_3(x) = \\left\\vert \\left \\lfloor{x}\\right \\rfloor  - 50 \\right\\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return (2 + np.sin(50*x)/50)*(np.arctan(x))**2\n",
    "\n",
    "def f3(x):\n",
    "    return np.abs(np.floor(x) - 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize f2\n",
    "visualize_opt_1d(f2, interval=(-5, -4, 6), niter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize f3\n",
    "visualize_opt_1d(f3, interval=(30, 35, 70), niter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the algorithm works in more than one dimension, we first need to discuss the simplex transformations in detail. The main idea, is that the simplex evolves at each iteration in order to explore the landscape of the function in a directed search towards the minimum. The simplex adapts itself to the local landscape, elongating down long inclined planes, changing direction on encountering a valley at an angle, and contracting in the neighborhood of a minimum.\n",
    "\n",
    "Nelder-Mead starts off with a randomly-generated simplex. At every iteration, it proceeds to reshape/move this simplex, one vertex at a time, towards an optimal region in the search space. During each step, it basically tries out one or a few modifications to the current simplex, and chooses one that shifts it towards a 'better' region of the domain. In an ideal case, the last few iterations of this algorithm would involve the simplex shrinking inwards towards the best point inside it. At the end, the vertex of the simplex that yields that most optimal objective value, is returned. The general algorithm is given below.\n",
    "\n",
    "- Construct the initial working simplex S using n+1 points (n is the number of dimensions) around the initial guess $x_0$. The exact method of creating the initial simplex is implementation dependent.\n",
    "- Repeat the following steps until the termination test is satisfied:\n",
    "  - calculate the termination test information;\n",
    "  - if the termination test is not satisfied then transform the working simplex.\n",
    "- Return the best vertex of the current simplex S and the associated function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One iteration of the Nelder-Mead method consists of the following three steps:\n",
    "1. **Ordering**. All the points are ordered/sorted, such that the value of $f$ for the first point is the highest, and that for the last point is the lowest. Let the indices of the first(worst), second(second worst) and last(best) points be $h$, $s$, $l$ respectively.\n",
    "\n",
    "2. **Computing the Centroid**. Consider all points except the worst ($x_h$). Compute their centroid(mean) as:\n",
    "$$\n",
    "c = \\frac{1}{n}\\sum_{i\\neq h} x_i\n",
    "$$\n",
    "\n",
    "3. **Transformation**. This is the most important part of the whole process â€“ transforming the simplex. Transformation takes place in one of the following ways:\n",
    "  1. **Reflection**. This is the first type of transformation that is tried out. You compute the 'reflected' point as:\n",
    "  $$\n",
    "  x_r = c + \\alpha(c - x_h)\n",
    "  $$\n",
    "  $\\alpha$ is called the reflection parameter, and is usually equal to one. $x_r$ is essentially a point on the line joining $c$ and $x_h$, but located away from $x_h$. This is done in an attempt to move the simplex in a direction thats away from the sub-optimal region around $x_h$. For a 2-D problem, heres what reflection looks like:\n",
    "  \n",
    "  ![](images/neldermead_1.jpg)\n",
    "  \n",
    "  Now, if $f(x_s) > f(x_r) \\geq f(x_l)$ -- that is, that $x_r$ is better than the second-worst point, but not better than the current best, we just replace $x_h$ with $x_r$ in the simplex, and move on to the next iteration.\n",
    "  \n",
    "  2. **Expansion**. If our luck is great and the reflected point $x_r$ happens to be better than the current best ($f(x_r) < f(x_l)$), we try to explore the possibility further.  In other words, we move a little bit more in the direction of $x_r$ from c, in the hope of finding an even better solution. The expanded point is defined as:\n",
    "  $$\n",
    "  x_e = c + \\gamma(x_r - c)\n",
    "  $$\n",
    "  \\gamma is called the expansion parameter, and its value in most implementations is 2. Heres what 2-D Expansion looks like:\n",
    "  \n",
    "  ![](images/neldermead_2.jpg)\n",
    "  \n",
    "  We then replace $x_h$ with the better of the two points: $x_e$ and $x_r$ in the simplex. This is called 'greedy optimization' -- we have replaced the worst point with the better of the two options we had. Theres another variant called 'greedy expansion', which replaces $x_h$ with $x_e$, as long as $f(x_e) < f(x_l)$. This is done irrespective of whether $x_e$ is better than $x_r$. The intuition for this, comes from the fact that Expansion always leads to a bigger simplex, which means more exploration of the search space.\n",
    "  \n",
    "  3. **Contraction**. Suppose our reflection point was worst than $x_s$ (i.e. the second worst point). In that case, we need to realize that the direction defined by $x_r$ may not be the ideal one for moving. Therefore, we end up contracting our simplex. The contraction point x_c is defined as:\n",
    "  $$\n",
    "  x_c = c + \\beta(\\min(x_h, x_r) - c)\n",
    "  $$\n",
    "  The simplex contracts respect to the better of $x_h$ or $x_r$. $\\beta$ is called the contraction parameter, and is usually equal to 0.5. In essence, we try to go against our exploration, instead contracting the simplex. An 'inward' 2-D contraction looks like this: \n",
    "\n",
    "  ![](images/neldermead_3.jpg)\n",
    "  \n",
    "  If $f(x_c) < f(\\min(x_h, x_r))$, which means that the contracted point is better than the current-worst, then we replace $x_h$ with $x_c$ in the simplex. However, if this is not the case, then we go to our last resort: the Shrink contraction.\n",
    "  \n",
    "  4. **Shrink contraction**. In this case, we redefine the entire simplex. We only keep the best point ($x_l$), and define all others with respect to it and the previous points. The j$^{th}$ new point, will now be defined as:\n",
    "  $$\n",
    "  x_j = x_l + \\delta (x_j - x_l)\n",
    "  $$\n",
    "  \\delta is called the shrinkage parameter, and is equal to 0.5 in most implementations. What we are essentially doing with the above definition, is moving each point in the simplex towards the current best point, in the hope of converging onto the best neighborhood. A 2-D Shrinking transformation looks like:\n",
    "  \n",
    "  ![](images/neldermead_5.jpg)\n",
    "  \n",
    "4. **Termination**. Termination is usually reached when:\n",
    "\n",
    "  1. A given number of iterations is reached\n",
    "  2. The simplex reaches some minimum 'size' limit.\n",
    "  3. The current best solution reaches some favorable limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the algorithm to two-variable functions. We again a few test functions:\n",
    "$$\n",
    "\\begin{align}\n",
    "f_1(x, y) =& x^2 + y^2 + x\\sin{y} + y\\sin{x}\\\\\n",
    "f_2(x, y) =& (x^2 + y  - 11)^2 + (x + y^2 - 7)^2\\\\\n",
    "f_3(x, y) =& (1 - x)^2 + 100(y - x^2)^2\\\\\n",
    "f_4(x, y) =& 0.26(x^2+y^2) + 0.48 xy\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to work with the minimize function, functions must accept a vector as input.\n",
    "\n",
    "def f1(x):\n",
    "    return x[0]**2 + x[1]**2 + x[0]*np.sin(x[1]) + x[1]*np.sin(x[0])\n",
    "\n",
    "def f2(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "def f3(x):\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def f4(x):\n",
    "    return 0.26*(x[0]**2 + x[1]**2) + 0.048*x[0]*y\n",
    "    \n",
    "f_list = [f1, f2, f3, f4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "import matplotlib.colors\n",
    "\n",
    "# Now, lets optimize each one of them.\n",
    "\n",
    "# Define the grid.\n",
    "X = np.linspace(-6, 6, 50)\n",
    "Y = np.linspace(-6, 6, 50)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20,12))\n",
    "\n",
    "for f, ax in zip(f_list, axes.ravel()):\n",
    "    # This step is crucial, take your time understanding it, decompose it in new cells if needed.\n",
    "    Z = f(np.stack([X.ravel(), Y.ravel()], axis=0)).reshape(X.shape)\n",
    "    \n",
    "    # Plotting the minimum\n",
    "    # This a hack to represent the levels in logaritmically distributed space, so we may visualize the mins better.\n",
    "    # Not that important.\n",
    "    levs = np.geomspace(0.001, Z.max() - Z.min(), 20) + Z.min()\n",
    "\n",
    "    ax.contourf(X, Y, Z, levels=levs, alpha=0.75)\n",
    "    result = optimize.minimize(f, x0=[-4, 4], method=\"Nelder-Mead\")\n",
    "    ax.plot(result.x[0], result.x[1], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.final_simplex[0][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise**\n",
    "\n",
    "Modify the 1D visualization function in order to visualize the algorithm in 2D spaces. Now the simplex is a triangle and the final_simplex object is more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2259ae817f5750c3aaf4adc0ab8c80ae",
     "grade": true,
     "grade_id": "cell-309c46d37bfd229f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize_opt_2d(func, xinterval, yinterval, niter, method='Nelder-Mead'):\n",
    "    \"\"\" A visualization for the Nedler-Mead algorithm in 2D. It allows to explore each iteration of the algorithm.\n",
    "    INPUT:\n",
    "        - func: the function to minimize\n",
    "        - xinterval: interval to visualize, must contain the starting point x0, of the form (xa, x0, xb)\n",
    "        - yinterval: interval to visualize, must contain the starting point y0, of the form (ya, y0, yb)\n",
    "        - niter: number of iterations to perform\n",
    "    OUPUT:\n",
    "        - An step by step visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack interval\n",
    "    xa, x0, xb = xinterval\n",
    "    assert xa < x0 < xb\n",
    "    ya, y0, yb = yinterval\n",
    "    assert ya < y0 < yb\n",
    "    v0 = [x0, y0]\n",
    "    \n",
    "    \n",
    "    # Initialize figure, define the grid as before, X,Y, X and levs\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Apply method. To have access to the iteration, do this in an\n",
    "    # artificial way: allow the algorithm to iter only once\n",
    "    converged = False\n",
    "    for i in range(niter):\n",
    "        # Run optimizer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        fig = plt.figure(figsize=(15,9))\n",
    "        # Plot the current simplex\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        plt.show()\n",
    "        sleep(0.4)\n",
    "        \n",
    "        # Check if already converged\n",
    "        if result.success:\n",
    "            print(f'Converged at {i} iterations')\n",
    "            converged = True\n",
    "            break\n",
    "    if not converged:\n",
    "        print(f'Did not converge after {niter} iterations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different functions.\n",
    "visualize_opt_2d(f1, xinterval=[-6,-4,6], yinterval=[-6,4,6], niter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and textual sources:\n",
    "- The Scipy Lecture Notes, at https://scipy-lectures.org/.\n",
    "- https://www.wiwi.uni-wuerzburg.de/fileadmin/12010500/user_upload/skripte/ss10/CompEcon/cechap33.pdf\n",
    "- https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/\n",
    "- http://www.scholarpedia.org/article/Nelder-Mead_algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
